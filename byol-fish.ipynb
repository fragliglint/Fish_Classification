{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12536942,"sourceType":"datasetVersion","datasetId":7914707}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ============================================================\n# STEP 0: Imports\n# ============================================================\nimport os\nimport shutil\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.models import resnet18\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-13T03:13:09.435465Z","iopub.execute_input":"2025-08-13T03:13:09.435669Z","iopub.status.idle":"2025-08-13T03:13:16.809796Z","shell.execute_reply.started":"2025-08-13T03:13:09.435652Z","shell.execute_reply":"2025-08-13T03:13:16.809263Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# ============================================================\n# STEP 1: Merge all RAW images into ONE folder (no subfolders)\n# ============================================================\nimport os\nimport shutil\n\ndataset_path = \"/kaggle/input/fish-classification-dataset/Fish Data\"\nmerged_path = \"/kaggle/working/fish_merged_raw\"\nos.makedirs(merged_path, exist_ok=True)\n\nfor cls in os.listdir(dataset_path):\n    cls_path = os.path.join(dataset_path, cls)\n    if not os.path.isdir(cls_path):\n        continue\n    \n    raw_folder = None\n    for name in os.listdir(cls_path):\n        if name.lower() == \"raw\" or name.lower() == \"raw data\":\n            raw_folder = os.path.join(cls_path, name)\n            break\n    \n    if raw_folder is None:\n        print(f\"⚠ No raw data found for class {cls}\")\n        continue\n    \n    for img in os.listdir(raw_folder):\n        src = os.path.join(raw_folder, img)\n        dst = os.path.join(merged_path, f\"{cls}_{img}\")  # prefix with class name to avoid name clashes\n        shutil.copy(src, dst)\n\nprint(\"✅ Merging complete. All raw images saved at:\", merged_path)\nprint(\"Total images:\", len(os.listdir(merged_path)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T03:13:16.811432Z","iopub.execute_input":"2025-08-13T03:13:16.811704Z","iopub.status.idle":"2025-08-13T03:18:02.347848Z","shell.execute_reply.started":"2025-08-13T03:13:16.811687Z","shell.execute_reply":"2025-08-13T03:18:02.347259Z"}},"outputs":[{"name":"stdout","text":"✅ Merging complete. All raw images saved at: /kaggle/working/fish_merged_raw\nTotal images: 26950\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ============================================================\n# STEP 2: BYOL Transform\n# ============================================================\nclass BYOLTransform:\n    def __init__(self):\n        self.base_transform = transforms.Compose([\n            transforms.RandomResizedCrop(224),\n            transforms.RandomHorizontalFlip(),\n            transforms.ColorJitter(0.4, 0.4, 0.4, 0.1),\n            transforms.RandomGrayscale(p=0.2),\n            transforms.ToTensor(),\n            transforms.Normalize((0.5,), (0.5,))\n        ])\n    \n    def __call__(self, x):\n        return self.base_transform(x), self.base_transform(x)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T03:18:02.348599Z","iopub.execute_input":"2025-08-13T03:18:02.348873Z","iopub.status.idle":"2025-08-13T03:18:02.353432Z","shell.execute_reply.started":"2025-08-13T03:18:02.348843Z","shell.execute_reply":"2025-08-13T03:18:02.352714Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from PIL import Image\nfrom torch.utils.data import Dataset\nimport glob\n\nclass FlatImageDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.files = glob.glob(os.path.join(root_dir, \"*\"))\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.files)\n    \n    def __getitem__(self, idx):\n        img_path = self.files[idx]\n        img = Image.open(img_path).convert(\"RGB\")\n        \n        if self.transform:\n            img1, img2 = self.transform(img)\n            return img1, img2\n        \n        return img\n\n# Now replace train_dataset and train_loader:\ntrain_dataset = FlatImageDataset(root_dir=merged_path, transform=BYOLTransform())\ntrain_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4)\n\nprint(\"Total images loaded:\", len(train_dataset))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T03:18:02.354148Z","iopub.execute_input":"2025-08-13T03:18:02.354409Z","iopub.status.idle":"2025-08-13T03:18:02.495612Z","shell.execute_reply.started":"2025-08-13T03:18:02.354391Z","shell.execute_reply":"2025-08-13T03:18:02.494978Z"}},"outputs":[{"name":"stdout","text":"Total images loaded: 26950\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ============================================================\n# STEP 4: BYOL Model Definition\n# ============================================================\nclass BYOL(nn.Module):\n    def __init__(self, encoder, projection_dim=256):\n        super().__init__()\n        self.online_encoder = encoder\n        self.online_projector = nn.Sequential(\n            nn.Linear(512, projection_dim),\n            nn.BatchNorm1d(projection_dim),\n            nn.ReLU(),\n            nn.Linear(projection_dim, projection_dim)\n        )\n        \n        self.target_encoder = resnet18(pretrained=False)\n        self.target_encoder.fc = nn.Identity()\n        self.target_projector = nn.Sequential(\n            nn.Linear(512, projection_dim),\n            nn.BatchNorm1d(projection_dim),\n            nn.ReLU(),\n            nn.Linear(projection_dim, projection_dim)\n        )\n        \n        for param in self.target_encoder.parameters():\n            param.requires_grad = False\n        for param in self.target_projector.parameters():\n            param.requires_grad = False\n        \n        self.criterion = nn.MSELoss()\n    \n    def forward(self, x1, x2):\n        o1 = self.online_projector(self.online_encoder(x1))\n        o2 = self.online_projector(self.online_encoder(x2))\n        \n        with torch.no_grad():\n            t1 = self.target_projector(self.target_encoder(x1))\n            t2 = self.target_projector(self.target_encoder(x2))\n        \n        loss = self.criterion(o1, t2) + self.criterion(o2, t1)\n        return loss\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T03:18:02.496217Z","iopub.execute_input":"2025-08-13T03:18:02.496389Z","iopub.status.idle":"2025-08-13T03:18:02.503121Z","shell.execute_reply.started":"2025-08-13T03:18:02.496374Z","shell.execute_reply":"2025-08-13T03:18:02.502387Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# ============================================================\n# STEP 5: Training Setup\n# ============================================================\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nencoder = resnet18(pretrained=False)\nencoder.fc = nn.Identity()\n\nmodel = BYOL(encoder).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\nscaler = torch.cuda.amp.GradScaler()\n\nEPOCHS = 30\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T03:18:02.503725Z","iopub.execute_input":"2025-08-13T03:18:02.504058Z","iopub.status.idle":"2025-08-13T03:18:03.113574Z","shell.execute_reply.started":"2025-08-13T03:18:02.504034Z","shell.execute_reply":"2025-08-13T03:18:03.112840Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n  warnings.warn(msg)\n/tmp/ipykernel_36/2526732460.py:10: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler()\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"model.train()\nfor epoch in range(EPOCHS):\n    total_loss = 0\n    loop = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{EPOCHS}]\")\n    for img1, img2 in loop:  # <-- no label unpacking here\n        img1, img2 = img1.to(device), img2.to(device)\n        \n        with torch.cuda.amp.autocast():\n            loss = model(img1, img2)\n        \n        optimizer.zero_grad()\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        \n        total_loss += loss.item()\n        loop.set_postfix(loss=loss.item())\n    \n    avg_loss = total_loss / len(train_loader)\n    print(f\"Epoch {epoch+1} Loss: {avg_loss:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T03:18:03.115519Z","iopub.execute_input":"2025-08-13T03:18:03.115772Z"}},"outputs":[{"name":"stderr","text":"Epoch [1/30]:   0%|          | 0/211 [00:00<?, ?it/s]/tmp/ipykernel_36/1047007721.py:8: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\nEpoch [1/30]: 100%|██████████| 211/211 [04:07<00:00,  1.17s/it, loss=0.23] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 Loss: 0.2504\n","output_type":"stream"},{"name":"stderr","text":"Epoch [2/30]: 100%|██████████| 211/211 [04:06<00:00,  1.17s/it, loss=0.233]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 Loss: 0.2342\n","output_type":"stream"},{"name":"stderr","text":"Epoch [3/30]: 100%|██████████| 211/211 [04:07<00:00,  1.17s/it, loss=0.255]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 Loss: 0.2329\n","output_type":"stream"},{"name":"stderr","text":"Epoch [4/30]: 100%|██████████| 211/211 [04:03<00:00,  1.15s/it, loss=0.235]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 Loss: 0.2319\n","output_type":"stream"},{"name":"stderr","text":"Epoch [5/30]: 100%|██████████| 211/211 [04:00<00:00,  1.14s/it, loss=0.225]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 Loss: 0.2306\n","output_type":"stream"},{"name":"stderr","text":"Epoch [6/30]: 100%|██████████| 211/211 [03:58<00:00,  1.13s/it, loss=0.238]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6 Loss: 0.2297\n","output_type":"stream"},{"name":"stderr","text":"Epoch [7/30]: 100%|██████████| 211/211 [03:58<00:00,  1.13s/it, loss=0.224]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7 Loss: 0.2299\n","output_type":"stream"},{"name":"stderr","text":"Epoch [8/30]:  14%|█▍        | 30/211 [00:37<03:23,  1.13s/it, loss=0.209]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# STEP 7: Save the Encoder\n# ============================================================\ntorch.save(model.online_encoder.state_dict(), \"/kaggle/working/byol_encoder.pth\")\nprint(\"✅ Encoder saved to /kaggle/working/byol_encoder.pth\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport random\nimport shutil\n\noriginal_path = \"/kaggle/input/fish-classification-dataset/Fish Data\"\nsplit_root = \"/kaggle/working/fish_split\"\n\ndef create_train_val_test_split(src_folder, dest_folder, train_ratio=0.7, val_ratio=0.15):\n    os.makedirs(dest_folder, exist_ok=True)\n    splits = ['train', 'val', 'test']\n    for s in splits:\n        os.makedirs(os.path.join(dest_folder, s), exist_ok=True)\n\n    for cls in os.listdir(src_folder):\n        cls_path = os.path.join(src_folder, cls)\n        if not os.path.isdir(cls_path):\n            continue\n        \n        # Find raw or raw data folder\n        raw_folder = None\n        for name in os.listdir(cls_path):\n            if name.lower() in (\"raw\", \"raw data\"):\n                raw_folder = os.path.join(cls_path, name)\n                break\n        if raw_folder is None:\n            continue\n\n        imgs = [f for f in os.listdir(raw_folder) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n        random.shuffle(imgs)\n\n        n = len(imgs)\n        n_train = int(n * train_ratio)\n        n_val = int(n * val_ratio)\n\n        splits_data = {\n            'train': imgs[:n_train],\n            'val': imgs[n_train:n_train + n_val],\n            'test': imgs[n_train + n_val:]\n        }\n\n        for split_name, img_list in splits_data.items():\n            dest_cls_dir = os.path.join(dest_folder, split_name, cls)\n            os.makedirs(dest_cls_dir, exist_ok=True)\n            for img_name in img_list:\n                src_img_path = os.path.join(raw_folder, img_name)\n                dst_img_path = os.path.join(dest_cls_dir, img_name)\n                shutil.copy(src_img_path, dst_img_path)\n\n    print(f\"Train/val/test split created at {dest_folder}\")\n\ncreate_train_val_test_split(original_path, split_root)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\neval_transform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])\n\ntrain_ds = datasets.ImageFolder(root=os.path.join(split_root, \"train\"), transform=eval_transform)\nval_ds = datasets.ImageFolder(root=os.path.join(split_root, \"val\"), transform=eval_transform)\ntest_ds = datasets.ImageFolder(root=os.path.join(split_root, \"test\"), transform=eval_transform)\n\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_ds, batch_size=64, shuffle=False, num_workers=4)\ntest_loader = DataLoader(test_ds, batch_size=64, shuffle=False, num_workers=4)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision.models import resnet18\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nencoder = resnet18(pretrained=False)\nencoder.fc = nn.Identity()\nencoder.load_state_dict(torch.load(\"/kaggle/working/byol_encoder.pth\"))\nencoder = encoder.to(device)\nencoder.eval()\n\nfor param in encoder.parameters():\n    param.requires_grad = False\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_classes = len(train_ds.classes)\nlinear_clf = nn.Linear(512, num_classes).to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(linear_clf.parameters(), lr=1e-3)\n\nepochs = 30\n\nfor epoch in range(epochs):\n    linear_clf.train()\n    total_loss = 0\n    for imgs, labels in train_loader:\n        imgs, labels = imgs.to(device), labels.to(device)\n\n        with torch.no_grad():\n            feats = encoder(imgs)\n\n        outputs = linear_clf(feats)\n        loss = criterion(outputs, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    avg_loss = total_loss / len(train_loader)\n    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n\n    # Validation accuracy\n    linear_clf.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for imgs, labels in val_loader:\n            imgs, labels = imgs.to(device), labels.to(device)\n            feats = encoder(imgs)\n            outputs = linear_clf(feats)\n            preds = outputs.argmax(dim=1)\n            correct += (preds == labels).sum().item()\n            total += labels.size(0)\n\n    print(f\"Validation Accuracy: {correct/total:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\n\nlinear_clf.eval()\ny_true = []\ny_pred = []\n\nwith torch.no_grad():\n    for imgs, labels in test_loader:\n        imgs, labels = imgs.to(device), labels.to(device)\n        feats = encoder(imgs)\n        outputs = linear_clf(feats)\n        preds = outputs.argmax(dim=1)\n        y_true.extend(labels.cpu().tolist())\n        y_pred.extend(preds.cpu().tolist())\n\ntest_acc = accuracy_score(y_true, y_pred)\nprint(f\"Test Accuracy: {test_acc:.4f}\")\n\ncm = confusion_matrix(y_true, y_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=train_ds.classes)\nfig, ax = plt.subplots(figsize=(10,10))\ndisp.plot(ax=ax, xticks_rotation='vertical')\nplt.title(\"Confusion Matrix on Test Set\")\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}